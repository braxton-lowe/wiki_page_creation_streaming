{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Spark session & context\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .master('local')\n",
    "         .appName('wiki-changes-event-consumer')\n",
    "         # Add kafka package\n",
    "         .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.5\")\n",
    "         .getOrCreate())\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stream read\n",
    "df = (spark\n",
    "  .readStream\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka-server:9092\") # kafka server\n",
    "  .option(\"subscribe\", \"wiki-changes\") # topic\n",
    "  .option(\"startingOffsets\", \"earliest\") # start from beginning \n",
    "  .load())\n",
    "\n",
    "# regular read to keep it simple before we stream\n",
    "# df = (spark\n",
    "#   .read\n",
    "#   .format(\"kafka\")\n",
    "#   .option(\"kafka.bootstrap.servers\", \"kafka-server:9092\") # kafka server\n",
    "#   .option(\"subscribe\", \"wiki-changes\") # topic\n",
    "#   .option(\"startingOffsets\", \"earliest\") # start from beginning \n",
    "#   .load())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Convert binary to string key and value\n",
    "df = df.withColumn(\"value\", df[\"value\"].cast(StringType()))\n",
    "\n",
    "# only run these for static df\n",
    "# df.show(2)\n",
    "# df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get schema from the binary value turned into to string\n",
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.types import StructType, StructField, BooleanType, LongType, IntegerType\n",
    "\n",
    "df_schema = StructType(\n",
    "    [StructField(\"performer\",\n",
    "                 StructType(\n",
    "                     [StructField(\"user_id\",IntegerType(),True),\n",
    "                      StructField(\"user_text\",StringType(),True),\n",
    "                      StructField(\"user_is_bot\",StringType(),True),\n",
    "                      StructField(\"user_registration_dt\",StringType(),True),\n",
    "                      StructField(\"user_edit_count\",IntegerType(),True),\n",
    "                      StructField(\"request_id\",StringType(),True)]),True),\n",
    "     StructField(\"meta\",\n",
    "                 StructType(\n",
    "                     [StructField(\"dt\",StringType(),True)]),True),\n",
    "    StructField(\"database\", StringType(),True)]\n",
    ")\n",
    "\n",
    "# Create dataframe setting schema for event data\n",
    "df_with_schema = df.withColumn(\"value\", from_json(\"value\", df_schema))\n",
    "\n",
    "# only run these for static df\n",
    "# df_with_schema.select('value').show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, from_unixtime, to_date, to_timestamp\n",
    "\n",
    "df3 = df_with_schema.select(\n",
    "    to_timestamp(col(\"value.meta.dt\")).alias(\"page_create_event_time\"),\n",
    "    col(\"value.performer.user_id\").alias(\"user_id\"),\n",
    "    col(\"value.performer.user_is_bot\").alias(\"user_isa_bot\"),\n",
    "    to_timestamp(col(\"value.performer.user_registration_dt\")).alias(\"date_user_registered\"),\n",
    "    col(\"value.performer.user_edit_count\").alias(\"num_user_edits\"),\n",
    "    col(\"value.database\").alias(\"wiki_db\"),\n",
    ")\n",
    "\n",
    "# only run these for static df\n",
    "# df3.show(2, truncate=False)\n",
    "# df3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start query stream over stream dataframe\n",
    "raw_path = \"/home/jovyan/work/data-lake/wiki-changes\"\n",
    "checkpoint_path = \"/home/jovyan/work/data-lake/wiki-changes-checkpoint\"\n",
    "\n",
    "queryStream =(\n",
    "    df3\n",
    "    .writeStream\n",
    "    .format(\"parquet\")\n",
    "    .queryName(\"wiki_changes_ingestion\")\n",
    "    .option(\"checkpointLocation\", checkpoint_path)\n",
    "    .option(\"path\", raw_path)\n",
    "    .outputMode(\"append\")\n",
    "    .partitionBy(\"wiki_db\")\n",
    "    .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read parquet files as stream\n",
    "df_wiki_changes = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"parquet\")\n",
    "    .schema(df3.schema)\n",
    "    .load(raw_path)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output to memory to count rows\n",
    "queryStreamMem = (df_wiki_changes\n",
    " .writeStream\n",
    " .format(\"memory\")\n",
    " .queryName(\"wiki_changes_count\")\n",
    " .outputMode(\"update\")\n",
    " .start())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run:10\n",
      "+---+\n",
      "|qty|\n",
      "+---+\n",
      "| 93|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Count rows every 5 seconds while stream is active\n",
    "try:\n",
    "    i=1\n",
    "    # While stream is active, print count\n",
    "    while len(spark.streams.active) > 0:\n",
    "        \n",
    "        # Clear output\n",
    "        clear_output(wait=True)\n",
    "        print(\"Run:{}\".format(i))\n",
    "        \n",
    "        lst_queries = []\n",
    "        for s in spark.streams.active:\n",
    "            lst_queries.append(s.name)\n",
    "\n",
    "        # Verify if wiki_changes_count query is active before count\n",
    "        if \"wiki_changes_count\" in lst_queries:\n",
    "            # Count number of events\n",
    "            spark.sql(\"select count(1) as qty from wiki_changes_count\").show()\n",
    "        else:\n",
    "            print(\"'wiki_changes_count' query not found.\")\n",
    "\n",
    "        sleep(5)\n",
    "        i=i+1\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    # Stop Query Stream\n",
    "    queryStreamMem.stop()\n",
    "    \n",
    "    print(\"stream process interrupted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID:07051997-c2bb-4334-a738-2a6613989ed1 | NAME:wiki_changes_ingestion\n"
     ]
    }
   ],
   "source": [
    "# Check active streams\n",
    "if spark.streams.active:\n",
    "    print(\"ID:{} | NAME:{}\".format(s.id, s.name))\n",
    "else:\n",
    "    print('all done here')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End\n",
    "queryStream.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
